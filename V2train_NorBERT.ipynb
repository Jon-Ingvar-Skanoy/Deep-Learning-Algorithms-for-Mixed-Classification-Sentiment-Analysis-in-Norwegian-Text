{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf00378-a443-45f9-901f-67e1fad3fb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import get_scheduler\n",
    "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
    "torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7387a791",
   "metadata": {},
   "source": [
    "The program can work with any of the four datasets with minor modifications. This program is adapted for the relabeled version of the mixed-label dataset. The program is written and commented with the help of ChatGPT and Copilot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2824b3-f8c1-40ad-b0c8-9fab68963fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Statistikkprosjekt/Mixed\")\n",
    "#dataset = load_dataset(\"ltg/norec_sentence\",\"ternary\")\n",
    "num_classes = 4\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0661068-180c-4467-88a0-a6da6630c796",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ltg/norbert3-base\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"review\"], padding=\"max_length\", truncation=True, max_length = 90, return_tensors=\"pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99a2741",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "num_epochs = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798ad752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models is where the BiGRU model will be saved \n",
    "\n",
    "\n",
    "models = [\n",
    "     \"example_folder/example_model1.pth\", \"example_folder/example_model2.pth\", \"example_folder/example_model3.pth\", \"example_folder/example_model4.pth\", \"example_folder/example_model5.pth\", \"example_folder/example_model6.pth\", \"example_folder/example_model7.pth\", \"example_folder/example_model8.pth\", \"example_folder/example_model9.pth\", \"example_folder/example_model10.pth\", \"example_folder/example_model11.pth\", \"example_folder/example_model12.pth\", \"example_folder/example_model13.pth\", \"example_folder/example_model14.pth\", \"example_folder/example_model15.pth\"\n",
    "          ]\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f603e1",
   "metadata": {},
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a3a245",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in models:\n",
    "    last = 130\n",
    "    print(i)\n",
    "    best = 240\n",
    " \n",
    "\n",
    "    tokenized_datasett = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    tokenized_datasett  = tokenized_datasett.remove_columns([\"review\"])\n",
    "\n",
    "\n",
    "    tokenized_datasett = tokenized_datasett.rename_column(\"polarity\", \"labels\")\n",
    "  \n",
    "\n",
    "    tokenized_datasett.set_format(\"torch\")\n",
    "  \n",
    "    small_train_dataset = tokenized_datasett[\"train\"].shuffle()\n",
    "    small_eval_dataset = tokenized_datasett[\"validation\"]\n",
    "\n",
    "    train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=14)\n",
    "    eval_dataloader = DataLoader(small_eval_dataset, batch_size=20)\n",
    "    num_training_steps = num_epochs * len(train_dataloader)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"ltg/norbert3-large\", trust_remote_code=True, num_labels=4, dropout=0.1, attention_probs_dropout_prob=0.1, hidden_dropout_prob=0.1)\n",
    "\n",
    "    # Initialize AdamW optimizer and learning rate scheduler\n",
    "    optimizer = AdamW(model.parameters(), lr=6e-6) \n",
    "    lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "    # Move model to the device (GPU)\n",
    "    model.to(device)\n",
    "    # lists for plotting\n",
    "    list_f1 = []\n",
    "    list_accuracy = []\n",
    "    list_train_loss = []\n",
    "    list_eval_loss = []\n",
    "    Counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        average_loss = 0   \n",
    "        model.train()\n",
    "        loss_train = 0\n",
    "        for batch in train_dataloader:\n",
    "            \n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            loss.backward()\n",
    "            loss_train += loss.item()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        average_loss = loss_train / len(train_dataloader)\n",
    "        list_train_loss.append(average_loss)\n",
    "        model.eval()\n",
    "        total_loss = 0.0\n",
    "        total_samples = 0\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        for batch in eval_dataloader:\n",
    "                \n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "               \n",
    "                with torch.no_grad():\n",
    "                    outputs = model(**batch)\n",
    "\n",
    "                logits = outputs.logits\n",
    "                predictions = torch.argmax(logits, dim=-1)\n",
    "                all_predictions.extend(predictions.cpu().numpy())\n",
    "                all_labels.extend(batch['labels'].cpu().numpy())\n",
    "\n",
    "                total_loss += outputs.loss.item() * batch['labels'].size(0)  # Access size of 'images' tensor \n",
    "                total_samples += batch['labels'].size(0)\n",
    "\n",
    "\n",
    "            # Calculate average loss over all samples\n",
    "        f1 = f1_score(all_labels, all_predictions, average='macro')\n",
    "        ac = accuracy_score(all_labels, all_predictions)\n",
    "        list_f1.append(f1)\n",
    "        list_accuracy.append(ac)\n",
    "        average_loss = total_loss / total_samples\n",
    "        list_eval_loss.append(average_loss)\n",
    "        print(f'Evaluation Loss: {average_loss}')    \n",
    "\n",
    " \n",
    "        \n",
    "\n",
    "        if average_loss < last:\n",
    "            \n",
    "            Counter = 0\n",
    "            if average_loss < best:\n",
    "                torch.save(model, i)\n",
    "                best = average_loss\n",
    "           \n",
    "        last = average_loss\n",
    "        Counter +=1\n",
    "        if Counter > 2:   \n",
    "            print(epoch)         \n",
    "            break\n",
    "\n",
    "    model = torch.load(i)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # Disable gradient computation during evaluation\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "\n",
    "            # Forward pass\n",
    "            \n",
    "            logits = outputs.logits\n",
    "            # Calculate predictions\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "            # Collect predictions and labels\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(batch[\"labels\"].cpu().numpy())\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(all_labels, all_predictions, average='macro')\n",
    "\n",
    "    print(f\"Test F1 Score: {f1:.4f}\")\n",
    "\n",
    "    # Optionally, you can print a detailed classification report\n",
    "    report = classification_report(all_labels, all_predictions)\n",
    "    print(\"Classification Report:\\n\", report)\n",
    "        \n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90d1f0a-4468-43de-abda-ce3b5038f3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(models[0])\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd1352b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "# Disable gradient computation during evaluation\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "\n",
    "        # Forward pass\n",
    "        \n",
    "        logits = outputs.logits\n",
    "        # Calculate predictions\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "        # Collect predictions and labels\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(batch[\"labels\"].cpu().numpy())\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = accuracy_score(all_labels, all_predictions)\n",
    "\n",
    "print(f\"Test ac Score: {f1:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "# Display the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "c = sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Neutral\",\"Positive\",\"Negative\",\"Mixed\"], yticklabels=[\"Neutral\",\"Positive\",\"Negative\",\"Mixed\"])\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "c.collections[0].colorbar.remove()\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show() \n",
    "\n",
    "# Optionally, you can print a detailed classification report\n",
    "report = classification_report(all_labels, all_predictions)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4583e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percentages for each true label\n",
    "total_true_labels = np.sum(conf_matrix, axis=1)\n",
    "percentages = (conf_matrix / total_true_labels[:, np.newaxis]) * 100\n",
    "\n",
    "# Replace NaN values with 0 (for cases where the true label count is 0)\n",
    "percentages = np.nan_to_num(percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ce2126",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sns.heatmap(percentages, annot=True, fmt=\"f\", cmap=\"Blues\", xticklabels=[\"Neutral\",\"Positive\",\"Negative\",\"Mixed\"], yticklabels=[\"Neutral\",\"Positive\",\"Negative\",\"Mixed\"])\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "\n",
    "a.collections[0].colorbar.remove()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40018959-fc50-47f5-95a9-ee507adda31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(list_f1, columns = ['f1'])\n",
    "df['accuracy'] = list_accuracy\n",
    "df['train_loss'] = list_train_loss\n",
    "df['eval_loss'] = list_eval_loss\n",
    "df['epoch'] = range(1,11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6514fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt = px.line(df, x='epoch', y='f1', title='F1 Macro Score Across Epochs on Validation Set of Mixed-Label Dataset',\n",
    "              labels={\"epoch\": \"Number of Epochs\", \"f1\": \"Macro F1\"}, \n",
    "              template='plotly_white')\n",
    "\n",
    "plt.update_traces(line=dict(width=2.5, color='darkred'), \n",
    "                  mode='lines', \n",
    "                  marker=dict(size=8, color='LightSkyBlue', line=dict(width=2, color='DarkSlateGrey')))\n",
    "plt.update_layout(title_font_size=24, title_x=0.5, \n",
    "                  xaxis_title_font=dict(size=18), yaxis_title_font=dict(size=18),\n",
    "                  xaxis_gridcolor='gray', yaxis_gridcolor='gray')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9bc894",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt = px.line(df, x='epoch', y='accuracy', title='Accuracy  Across Epochs on Validation Set of Mixed-Label Dataset',\n",
    "              labels={\"epoch\": \"Number of Epochs\", \"accuracy\": \"Accuracy\"},\n",
    "              template='plotly_white')\n",
    "\n",
    "plt.update_traces(line=dict(width=2.5, color='darkred'), \n",
    "                  mode='lines', \n",
    "                  marker=dict(size=8, color='LightSkyBlue', line=dict(width=2, color='DarkSlateGrey')))\n",
    "plt.update_layout(title_font_size=24, title_x=0.5, \n",
    "                  xaxis_title_font=dict(size=18), yaxis_title_font=dict(size=18),\n",
    "                  xaxis_gridcolor='gray', yaxis_gridcolor='gray')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3e6355",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt = px.line(df, x='epoch', y='train_loss', title='Loss Across Epochs on Train Set of Mixed-Label Dataset',\n",
    "              labels={\"epoch\": \"Number of Epochs\", \"train_loss\": \"Training Loss\"},\n",
    "              template='plotly_white')\n",
    "\n",
    "plt.update_traces(line=dict(width=2.5, color='darkred'), \n",
    "                  mode='lines', \n",
    "                  marker=dict(size=8, color='LightSkyBlue', line=dict(width=2, color='DarkSlateGrey')))\n",
    "plt.update_layout(title_font_size=24, title_x=0.5, \n",
    "                  xaxis_title_font=dict(size=18), yaxis_title_font=dict(size=18),\n",
    "                  xaxis_gridcolor='gray', yaxis_gridcolor='gray')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f71afe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt = px.line(df, x='epoch', y='eval_loss', title='Evaluation Loss Across Epochs on Validation Set of Mixed-Label Dataset',\n",
    "              labels={\"epoch\": \"Number of Epochs\", \"eval_loss\": \"Evaluation Loss\"},\n",
    "              template='plotly_white')\n",
    "\n",
    "plt.update_traces(line=dict(width=2.5, color='darkred'), \n",
    "                  mode='lines', \n",
    "                  marker=dict(size=8, color='LightSkyBlue', line=dict(width=2, color='DarkSlateGrey')))\n",
    "plt.update_layout(title_font_size=24, title_x=0.5, \n",
    "                  xaxis_title_font=dict(size=18), yaxis_title_font=dict(size=18),\n",
    "                  xaxis_gridcolor='gray', yaxis_gridcolor='gray')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b19fb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
