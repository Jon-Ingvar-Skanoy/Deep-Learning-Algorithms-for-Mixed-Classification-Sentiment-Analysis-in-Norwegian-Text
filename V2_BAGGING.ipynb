{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np \n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomGRU(nn.Module):\n",
    "    def __init__(self, bert_hidden_size, gru_hidden_size, num_classes):\n",
    "        super(CustomGRU, self).__init__()\n",
    "\n",
    "        # GRU layer\n",
    "        self.gru = nn.GRU(bert_hidden_size, gru_hidden_size, batch_first=True,\n",
    "                          bidirectional=False, dropout=0.3, num_layers=3)\n",
    "\n",
    "\n",
    "        # Fully connected layer for classification.\n",
    "        self.fc = nn.Linear(gru_hidden_size, num_classes)\n",
    "\n",
    "  \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # GRU layer.\n",
    "        gru_out, _ = self.gru(x)\n",
    "        \n",
    "        # Selecting the output from the last time step of all sequences.\n",
    "        x = gru_out[:, -1, :]\n",
    "\n",
    "\n",
    "        # Output layer.\n",
    "        output = self.fc(x)\n",
    "\n",
    "\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomLSTM(nn.Module):\n",
    "    def __init__(self, bert_hidden_size, lstm_hidden_size, num_classes):\n",
    "        super(CustomLSTM, self).__init__()\n",
    "\n",
    "        # Bidirectional LSTM layer with dropout and multiple layers.\n",
    "        self.lstm = nn.LSTM(bert_hidden_size, lstm_hidden_size, batch_first=True,\n",
    "                            bidirectional=True, num_layers=1)\n",
    "\n",
    "    \n",
    "        # Fully connected layer for classification.\n",
    "        self.fc = nn.Linear(lstm_hidden_size*2, num_classes)\n",
    "\n",
    "        # Dropout layer to prevent overfitting.\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "\n",
    "        # LSTM layer.\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "\n",
    "        # Selecting the output from the last time step of all sequences.\n",
    "        x = lstm_out[:, -1, :]\n",
    "\n",
    "     \n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Output layer.\n",
    "        output = self.fc(x)\n",
    "\n",
    "\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_NLP(nn.Module):\n",
    " \n",
    "    def __init__(self,\n",
    "                 \n",
    "                 vocab_size=1024,\n",
    "            \n",
    "                 filter_sizes=[3, 4, 5],\n",
    "                 num_filters=[100, 100, 100],\n",
    "                 num_classes=4,\n",
    "                 dropout=0.5):\n",
    "\n",
    "\n",
    "        super(CNN_NLP, self).__init__()\n",
    "\n",
    "    \n",
    "        # Conv Network\n",
    "        self.conv1d_list = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=vocab_size,\n",
    "                      out_channels=num_filters[i],\n",
    "                      kernel_size=filter_sizes[i])\n",
    "            for i in range(len(filter_sizes))\n",
    "        ])\n",
    "        # Fully-connected layer and Dropout\n",
    "        self.fc = nn.Linear(np.sum(num_filters), num_classes)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "\n",
    "\n",
    "       \n",
    "        x_embed = input_ids.float()\n",
    "\n",
    "\n",
    "        x_reshaped = x_embed.permute(0, 2, 1)\n",
    "\n",
    "        # Apply CNN and ReLU. Output shape: (b, num_filters[i], L_out)\n",
    "        x_conv_list = [F.relu(conv1d(x_reshaped)) for conv1d in self.conv1d_list]\n",
    "\n",
    "        # Max pooling. Output shape: (b, num_filters[i], 1)\n",
    "        x_pool_list = [F.max_pool1d(x_conv, kernel_size=x_conv.shape[2])\n",
    "            for x_conv in x_conv_list]\n",
    "        \n",
    "        # Concatenate x_pool_list to feed the fully connected layer.\n",
    "        # Output shape: (b, sum(num_filters))\n",
    "        x_fc = torch.cat([x_pool.squeeze(dim=2) for x_pool in x_pool_list],\n",
    "                         dim=1)\n",
    "        \n",
    "        # Compute logits. Output shape: (b, n_classes)\n",
    "        logits = self.fc(self.dropout(x_fc))\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class CustomUNLSTM(nn.Module):\n",
    "    def __init__(self, bert_hidden_size, lstm_hidden_size, num_classes):\n",
    "        super(CustomUNLSTM, self).__init__()\n",
    "\n",
    "        # Bidirectional LSTM layer with dropout and multiple layers.\n",
    "        self.lstm = nn.LSTM(bert_hidden_size, lstm_hidden_size, batch_first=True,\n",
    "                            bidirectional=False, num_layers=1)\n",
    "\n",
    "       \n",
    "\n",
    "        # Fully connected layer for classification.\n",
    "        self.fc = nn.Linear(lstm_hidden_size, num_classes)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Reshape BERT embeddings.\n",
    "\n",
    "        # LSTM layer.\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "  \n",
    "        # Selecting the output from the last time step of all sequences.\n",
    "        x = lstm_out[:, -1, :]\n",
    "\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Output layer.\n",
    "        output = self.fc(x)\n",
    "\n",
    "    \n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomBIGRU(nn.Module):\n",
    "    def __init__(self, bert_hidden_size, gru_hidden_size, num_classes):\n",
    "        super(CustomBIGRU, self).__init__()\n",
    "\n",
    "        # Bidirectional LSTM layer with dropout and multiple layers.\n",
    "        self.gru = nn.GRU(bert_hidden_size, gru_hidden_size, batch_first=True,\n",
    "                          bidirectional=True, dropout=0.2, num_layers=3)\n",
    "\n",
    "        # Fully connected layer for classification.\n",
    "        self.fc = nn.Linear(gru_hidden_size * 2, num_classes)\n",
    "\n",
    "        # Dropout layer to prevent overfitting.\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "       \n",
    "\n",
    "        # GRU layer.\n",
    "        gru_out, _ = self.gru(x)\n",
    "        \n",
    "        # Selecting the output from the last time step of all sequences.\n",
    "        x = gru_out[:, -1, :]\n",
    "\n",
    " \n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Output layer.\n",
    "        output = self.fc(x)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The program can work with any of the four datasets with minor modifications. This program is adapted for the relabeled version of the mixed-label dataset. The program is written and commented with the help of ChatGPT and Copilot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "\n",
    "norbert_dataset = load_dataset(\"Statistikkprosjekt/Mixed\")\n",
    "T5_list = []\n",
    "for i in norbert_dataset[\"test\"]:\n",
    "    T5_list.append({\"review\": i[\"review\"], \"polarity\": str(i[\"polarity\"])})\n",
    "T5_dataset = DatasetDict({\"test\": Dataset.from_list(T5_list)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ltg/norbert3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"review\"], padding=\"max_length\", truncation=True, max_length = 90, return_tensors=\"pt\")\n",
    "def tokenize_label_function(examples):\n",
    "    return tokenizer(examples[\"polarity\"], truncation=True, max_length = 1, return_tensors=\"pt\")\n",
    "\n",
    "norbert_dataset = norbert_dataset.map(tokenize_function, batched=True)\n",
    "T5_dataset = T5_dataset.map(tokenize_label_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "T5_dataset = T5_dataset.rename_column(\"input_ids\",\"labels\")\n",
    "T5_dataset = T5_dataset.remove_columns([\"token_type_ids\",\"attention_mask\",\"polarity\"])\n",
    "T5_dataset = T5_dataset.map(tokenize_function, batched=True)\n",
    "T5_dataset = T5_dataset.remove_columns([\"review\",\"token_type_ids\"])\n",
    "\n",
    "\n",
    "norbert_dataset = norbert_dataset.remove_columns([\"review\"])\n",
    "norbert_dataset = norbert_dataset.rename_column(\"polarity\", \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "norbert_dataset.set_format(\"torch\")\n",
    "T5_dataset.set_format(\"torch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "norbert_dataset = norbert_dataset[\"test\"]\n",
    "T5_dataset = T5_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ltg/norbert3-base\")\n",
    "from transformers import AutoModel\n",
    "norbert = AutoModel.from_pretrained(\"ltg/norbert3-large\", trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = 10\n",
    "\n",
    "norbert_dataloader = DataLoader(norbert_dataset, batch_size=batch_size)\n",
    "T5_dataloader= DataLoader(T5_dataset,batch_size=batch_size)\n",
    "# \n",
    "model_files = [\n",
    "    #Example file names\n",
    "    \"norbert1.pth\", \"norbert2.pth\", \"norT5_1.pth\",\n",
    "    #Models that are embedded with a specific NorBERT needs to be in a list of length 2 with the NorBERT\n",
    "    [\"BiGRU1.pth\",\"NBERT_BIGRU1.pth,\"], ['CNN7.pth','NBERT_CNN7.pth']\n",
    "\n",
    "\n",
    "\n",
    "               ]\n",
    "len(model_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import precision_score\n",
    "import tqdm\n",
    "precision_metric = precision_score  # You can use other metrics suitable for your task\n",
    "\n",
    "# Initialize an array to store individual model predictions\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "all_logits = []\n",
    "\n",
    "a = 0\n",
    "for i in model_files:\n",
    "    torch.cuda.empty_cache()\n",
    "   \n",
    "    print(i)\n",
    "    predictions = []\n",
    "    logits_list = []\n",
    "    if \"norbert\" in i :\n",
    "        model = torch.load(i)\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        for batch in norbert_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "\n",
    "            \n",
    "            logits = outputs.logits \n",
    "            logits_list.append(logits.cpu().numpy())\n",
    "        \n",
    "\n",
    "            # Extract true labels for the current batch\n",
    "            if a == 0:\n",
    "                true_labels_batch = batch[\"labels\"].cpu().numpy()\n",
    "                all_true_labels.extend(true_labels_batch)\n",
    "    elif \"T5\" in i:\n",
    "            model = torch.load(i)\n",
    "            model.to(device)\n",
    "            model.eval()\n",
    "            for batch in T5_dataloader:\n",
    "                \n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                \n",
    "\n",
    "                            \n",
    "                with torch.no_grad():\n",
    "                    # Make sure to provide the correct input tensors for your model\n",
    "                    outputs = model(**batch)\n",
    "        \n",
    " \n",
    "                \n",
    "                # Process the logits according to your task\n",
    "                logits = outputs.logits\n",
    "                \n",
    "                indices = torch.tensor([920,561  ,414 , 395])\n",
    "                selected_elements = logits[:, 1, indices]\n",
    "                logits_list.append(selected_elements.cpu().numpy())\n",
    "               \n",
    "    else:\n",
    "        model = torch.load(i[0])\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        norbert = torch.load(i[1])\n",
    "        norbert.to(device)\n",
    "        norbert.eval()\n",
    "    \n",
    "        for batch in norbert_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "           \n",
    "\n",
    "            with torch.no_grad():\n",
    "                norbert_outputs = norbert(**batch)\n",
    "                \n",
    "                batch_hidden_states = norbert_outputs.last_hidden_state\n",
    "                outputs = model(batch_hidden_states)\n",
    "              \n",
    "            \n",
    "            logits = outputs\n",
    "            logits_list.append(logits.cpu().numpy())\n",
    "        \n",
    "\n",
    "            # Extract true labels for the current batch\n",
    "            if a == 0:\n",
    "                true_labels_batch = batch[\"labels\"].cpu().numpy()\n",
    "                all_true_labels.extend(true_labels_batch)\n",
    "        del norbert\n",
    "    \n",
    "\n",
    "    flattened_list = [result for batch in logits_list for result in batch]\n",
    "    all_logits.append(flattened_list)\n",
    "    a = 1\n",
    "\n",
    "\n",
    "\n",
    "#import winsound\n",
    "\n",
    "# Your program code goes here\n",
    "\n",
    "# Beep to indicate program completion\n",
    "#winsound.Beep(1000, 500)  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DENNE = np.array(all_logits)\n",
    "all_true_labels= np.array(all_true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_logits = DENNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "ac_list = []\n",
    "f1_list = []\n",
    "recall_list = []\n",
    "precision_list = []\n",
    "for i in all_logits:\n",
    "    tmp = np.array(i)\n",
    "    print(tmp.shape)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    bagging_predictions = np.argmax(tmp, axis=1)\n",
    "    #print(bagging_predictions.shape)\n",
    "\n",
    "    ac_list.append(accuracy_score(y_true=all_true_labels, y_pred=bagging_predictions))\n",
    "    f1_list.append(f1_score(y_true=all_true_labels, y_pred=bagging_predictions,average='macro'))\n",
    "    recall_list.append(recall_score(y_true=all_true_labels, y_pred=bagging_predictions,average='macro'))\n",
    "    precision_list.append(precision_score(y_true=all_true_labels, y_pred=bagging_predictions,average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lists = [ f1_list,ac_list, recall_list, precision_list]\n",
    "selection = 0\n",
    "mean = np.mean(lists[selection])*100\n",
    "std = np.std(lists[selection])*100\n",
    "\n",
    "# Print the result in the specified format\n",
    "for i in lists:\n",
    "    mean = np.mean(i)*100\n",
    "    std = np.std(i)*100\n",
    "\n",
    "    print(f\"${mean:.4f}^{{\\pm{std:.4f}}}$ &\",end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging_logits = np.mean(all_logits, axis=0)\n",
    "\n",
    "print(bagging_logits.shape)\n",
    "bagging_predictions = np.argmax(bagging_logits, axis=1)\n",
    "print(bagging_predictions.shape)\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(all_true_labels, bagging_predictions, average='macro')\n",
    "ac = accuracy_score(all_true_labels, bagging_predictions)\n",
    "recall = recall_score(all_true_labels, bagging_predictions, average='macro')\n",
    "precision = precision_score(all_true_labels, bagging_predictions, average='macro')\n",
    "print(f\"Test F1 Score: {f1:.3f}\")\n",
    "print(f\"Test Accuracy: {ac:.3f}\")\n",
    "print(f\"Test Recall: {recall:.3f}\")\n",
    "print(f\"Test Precision: {precision:.3f}\")\n",
    "for i in [f1,ac,recall,precision]:\n",
    "    \n",
    "\n",
    "    print(f\"${i*100:.4f}$ &\",end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "all_logits = np.array(DENNE)\n",
    "all_logits.shape\n",
    "# Get the shape of all_logits\n",
    "shape = all_logits.shape\n",
    "i =1\n",
    "f1_list = []\n",
    "ac_list = []\n",
    "recall_list = []\n",
    "precision_list = []\n",
    "\n",
    "# Select two random indices\n",
    "for i in range(1,16):\n",
    "    \n",
    "    f1_list.append([])\n",
    "    ac_list.append([])\n",
    "    recall_list.append([])\n",
    "    precision_list.append([])\n",
    "    for _ in range(5000):\n",
    "        indices = random.sample(range(shape[0]), i)\n",
    "        # Create a new tensor with shape (2, 1264, 4)\n",
    "        new_tensor = np.zeros((i, shape[1], shape[2]))\n",
    "\n",
    "        # Assign the selected indices from all_logits to the new tensor\n",
    "        for j in range(i):\n",
    "            new_tensor[j] = all_logits[indices[j]]\n",
    "\n",
    "        bagging_logits = np.mean(new_tensor, axis=0)\n",
    "        bagging_predictions = np.argmax(bagging_logits, axis=1)\n",
    "   \n",
    "        f1 = f1_score(all_true_labels, bagging_predictions, average='macro')\n",
    "        ac = accuracy_score(all_true_labels, bagging_predictions)\n",
    "        recall = recall_score(all_true_labels, bagging_predictions, average='macro')\n",
    "        precision = precision_score(all_true_labels, bagging_predictions, average='macro')\n",
    "        f1_list[i-1].append(f1)\n",
    "        ac_list[i-1].append(ac)\n",
    "        recall_list[i-1].append(recall)\n",
    "        precision_list[i-1].append(precision)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [f1_list[14],ac_list[14],recall_list[14],precision_list[14]]:\n",
    "    mean_list = []\n",
    "    std_list = []\n",
    "    mean = np.mean(i)*100\n",
    "    std = np.std(i)*100\n",
    "    mean_list.append(mean)\n",
    "    std_list.append(std)\n",
    "    print(f\"${mean:.4f}^{{\\pm{std:.4f}}}$ &\",end=\"\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [f1_list[4],ac_list[4],recall_list[4],precision_list[4]]:\n",
    "    mean_list = []\n",
    "    std_list = []\n",
    "    mean = np.mean(i)*100\n",
    "    std = np.std(i)*100\n",
    "    mean_list.append(mean)\n",
    "    std_list.append(std)\n",
    "    print(f\"${mean:.4f}^{{\\pm{std:.4f}}}$ &\",end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "import plotly.io as pio\n",
    "#io.renderers.default = 'png'\n",
    "mean_list = np.mean(f1_list, axis=1)\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(mean_list, columns = ['f1'])\n",
    "df['Number Of Models In Ensemble'] = range(1,16)\n",
    "plt = px.line(df, x='Number Of Models In Ensemble', y='f1', title='F1 Macro Score Across Ensemble Sizes on Test Set of Mixed-Label Dataset',\n",
    "              labels={\"epoch\": \"Epoch\", \"f1\": \"F1 Score\"}, \n",
    "              template='plotly')\n",
    "\n",
    "plt.update_traces(line=dict(width=2.5, color='darkred'), \n",
    "                  mode='lines', \n",
    "                  marker=dict(size=8, color='LightSkyBlue', line=dict(width=2, color='DarkSlateGrey')))\n",
    "plt.update_layout(title_font_size=24, title_x=0.5, \n",
    "                  xaxis_title_font=dict(size=18), yaxis_title_font=dict(size=18),\n",
    "                  xaxis_gridcolor='gray', yaxis_gridcolor='gray')\n",
    "#plt.to_image(format=\"png\", width=600, height=350, scale=2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_list = np.mean(ac_list, axis=1)\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(mean_list, columns = ['f1'])\n",
    "df['Number Of Models In Ensemble'] = range(1,16)\n",
    "plt = px.line(df, x='Number Of Models In Ensemble', y='f1', title='Accuracy Across Ensemble Sizes on Test Set of Mixed-Label Dataset',\n",
    "              labels={\"epoch\": \"Epoch\", \"f1\": \"Accuracy\"}, \n",
    "              template='plotly_white')\n",
    "\n",
    "plt.update_traces(line=dict(width=2.5, color='darkred'), \n",
    "                  mode='lines', \n",
    "                  marker=dict(size=8, color='LightSkyBlue', line=dict(width=2, color='DarkSlateGrey')))\n",
    "plt.update_layout(title_font_size=24, title_x=0.5, \n",
    "                  xaxis_title_font=dict(size=18), yaxis_title_font=dict(size=18),\n",
    "                  xaxis_gridcolor='gray', yaxis_gridcolor='gray')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_list = np.mean(recall_list, axis=1)\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(mean_list, columns = ['f1'])\n",
    "df['Number Of Models In Ensemble'] = range(1,16)\n",
    "plt = px.line(df, x='Number Of Models In Ensemble', y='f1', title='Recall Macro Score Across Ensemble Sizes on Test Set of Mixed-Label Dataset',\n",
    "              labels={\"epoch\": \"Epoch\", \"f1\": \"Recall\"}, \n",
    "              template='plotly_white')\n",
    "\n",
    "plt.update_traces(line=dict(width=2.5, color='darkred'), \n",
    "                  mode='lines', \n",
    "                  marker=dict(size=8, color='LightSkyBlue', line=dict(width=2, color='DarkSlateGrey')))\n",
    "plt.update_layout(title_font_size=24, title_x=0.5, \n",
    "                  xaxis_title_font=dict(size=18), yaxis_title_font=dict(size=18),\n",
    "                  xaxis_gridcolor='gray', yaxis_gridcolor='gray')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_list = np.mean(precision_list, axis=1)\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(mean_list, columns = ['f1'])\n",
    "df['Number Of Models In Ensemble'] = range(1,16)\n",
    "plt = px.line(df, x='Number Of Models In Ensemble', y='f1', title='Precision Macro Score Across Ensemble Sizes on Test Set of Mixed-Label Dataset',\n",
    "              labels={\"epoch\": \"Epoch\", \"f1\": \"Precision\"}, \n",
    "              template='plotly_white')\n",
    "\n",
    "plt.update_traces(line=dict(width=2.5, color='darkred'), \n",
    "                  mode='lines', \n",
    "                  marker=dict(size=8, color='LightSkyBlue', line=dict(width=2, color='DarkSlateGrey')))\n",
    "plt.update_layout(title_font_size=24, title_x=0.5, \n",
    "                  xaxis_title_font=dict(size=18), yaxis_title_font=dict(size=18),\n",
    "                  xaxis_gridcolor='gray', yaxis_gridcolor='gray')\n",
    "#plt.to_image(format=\"png\", width=600, height=350, scale=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(all_true_labels, bagging_predictions)\n",
    "\n",
    "# Display the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "c = sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Neutral\",\"Positive\",\"Negative\", \"mixed\"], yticklabels=[\"Neutral\",\"Positive\",\"Negative\", \"mixed\"])\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "c.collections[0].colorbar.remove()\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Calculate percentages for each true label\n",
    "total_true_labels = np.sum(conf_matrix, axis=1)\n",
    "percentages = (conf_matrix / total_true_labels[:, np.newaxis]) * 100\n",
    "\n",
    "# Replace NaN values with 0 (for cases where the true label count is 0)\n",
    "percentages = np.nan_to_num(percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "c = sns.heatmap(percentages, annot=True, fmt=\"f\", cmap=\"Blues\", xticklabels=[\"Neutral\",\"Positive\",\"Negative\", \"mixed\"], yticklabels=[\"Neutral\",\"Positive\",\"Negative\", \"mixed\"])\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "c.collections[0].colorbar.remove()\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
