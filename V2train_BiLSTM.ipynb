{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf00378-a443-45f9-901f-67e1fad3fb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "import torch.nn as nn\n",
    "from transformers import get_scheduler\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn.functional as F\n",
    "torch.cuda.is_available()\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29b3cf2",
   "metadata": {},
   "source": [
    "The program can work with any of the four datasets with minor modifications. This program is adapted for the relabeled version of the mixed-label dataset. The program is written and commented with the help of ChatGPT and Copilot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2824b3-f8c1-40ad-b0c8-9fab68963fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "datasett = load_dataset(\"Statistikkprosjekt/Mixed\")\n",
    "#datasett = load_dataset(\"ltg/norec_sentence\",\"ternary\")\n",
    "num_classes = 4\n",
    "datasett "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a15ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CustomLSTM(nn.Module):\n",
    "    def __init__(self, bert_hidden_size, lstm_hidden_size, num_classes):\n",
    "        super(CustomLSTM, self).__init__()\n",
    "\n",
    "        # Bidirectional LSTM layer with dropout and multiple layers.\n",
    "        self.lstm = nn.LSTM(bert_hidden_size, lstm_hidden_size, batch_first=True,\n",
    "                            bidirectional=True, num_layers=1)\n",
    "\n",
    "    \n",
    "        # Fully connected layer for classification.\n",
    "        self.fc = nn.Linear(lstm_hidden_size*2, num_classes)\n",
    "\n",
    "        # Dropout layer to prevent overfitting.\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "\n",
    "        # LSTM layer.\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "\n",
    "        # Selecting the output from the last time step of all sequences.\n",
    "        x = lstm_out[:, -1, :]\n",
    "\n",
    "     \n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Output layer.\n",
    "        output = self.fc(x)\n",
    "\n",
    "\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1616fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"ltg/norbert3-base\")\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"review\"], padding=\"max_length\", truncation=True, max_length = 90, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99a2741",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798ad752",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "     \"example_folder/example_model1.pth\", \"example_folder/example_model2.pth\", \"example_folder/example_model3.pth\", \"example_folder/example_model4.pth\", \"example_folder/example_model5.pth\", \"example_folder/example_model6.pth\", \"example_folder/example_model7.pth\", \"example_folder/example_model8.pth\", \"example_folder/example_model9.pth\", \"example_folder/example_model10.pth\", \"example_folder/example_model11.pth\", \"example_folder/example_model12.pth\", \"example_folder/example_model13.pth\", \"example_folder/example_model14.pth\", \"example_folder/example_model15.pth\"\n",
    "          ]\n",
    "        \n",
    "norbert_models = [\n",
    "    \"example_folder/example_norbert1.pth\", \"example_folder/example_norbert2.pth\", \"example_folder/example_norbert3.pth\", \"example_folder/example_norbert4.pth\", \"example_folder/example_norbert5.pth\", \"example_folder/example_norbert6.pth\", \"example_folder/example_norbert7.pth\", \"example_folder/example_norbert8.pth\", \"example_folder/example_norbert9.pth\", \"example_folder/example_norbert10.pth\", \"example_folder/example_norbert11.pth\", \"example_folder/example_norbert12.pth\", \"example_folder/example_norbert13.pth\", \"example_folder/example_norbert14.pth\", \"example_folder/example_norbert15.pth\"\n",
    "       ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a3a245",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j in zip( models, norbert_models):\n",
    "    print(i)\n",
    "    last = 199\n",
    "    best = 199\n",
    "    tokenized_datasets = datasett.map(tokenize_function, batched=True)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    tokenized_datasets = tokenized_datasets.remove_columns([\"review\"])\n",
    "    tokenized_datasets = tokenized_datasets.rename_column(\"polarity\", \"labels\")\n",
    "    tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    small_train_dataset = tokenized_datasets[\"train\"].shuffle()\n",
    "    small_eval_dataset = tokenized_datasets[\"validation\"]\n",
    "\n",
    "    train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=14)\n",
    "    eval_dataloader = DataLoader(small_eval_dataset, batch_size=30)\n",
    "    num_training_steps = 10 * len(train_dataloader)\n",
    "\n",
    "    norbert = AutoModel.from_pretrained(\"ltg/norbert3-large\", trust_remote_code=True)\n",
    "\n",
    "    text = \"Your input text goes here.\"\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "    # Forward pass to get BERT embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs_bert = norbert(**tokens)\n",
    "\n",
    "    # Get the output embeddings from the last layer\n",
    "    last_hidden_states = outputs_bert.last_hidden_state\n",
    "\n",
    "    # Assuming you want to use the [CLS] token embedding for each example\n",
    "    cls_embeddings = last_hidden_states[:, 0, :]\n",
    "\n",
    "    # Instantiate your BiLSTM model\n",
    "    bert_hidden_size = last_hidden_states.size(-1)\n",
    "    lstm_hidden_size = 512\n",
    "   \n",
    "    model = CustomLSTM(bert_hidden_size, lstm_hidden_size, num_classes)\n",
    "    # sett optimizer and scheduler\n",
    "    optimizer_norbert = AdamW(norbert.parameters(), lr=4e-6) \n",
    "    optimizer2 = AdamW(model.parameters(), lr=5e-6) \n",
    "    lr_scheduler_norbert = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer_norbert, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "    lr_scheduler2 = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer2, num_warmup_steps=0, num_training_steps=num_training_steps)\\\n",
    "    # move model to device\n",
    "    norbert.to(device)\n",
    "    model.to(device)\n",
    "\n",
    "   # training loop\n",
    "    for epoch in range(num_epochs):\n",
    "  \n",
    "        average_loss = 0   \n",
    "        norbert.train()\n",
    "        model.train()   \n",
    "        train_loss = 0\n",
    "        train_samples = 0\n",
    "        for batch in train_dataloader:\n",
    "            \n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = norbert(**batch)\n",
    "\n",
    "            batch_hidden_states = outputs.last_hidden_state\n",
    "            model_output =    model(batch_hidden_states)\n",
    "            loss = F.cross_entropy(model_output, batch[\"labels\"])\n",
    "      \n",
    "\n",
    "\n",
    "            train_loss += loss * batch['labels'].size(0)   \n",
    "            train_samples += batch['labels'].size(0)\n",
    "\n",
    "           \n",
    "            \n",
    "            loss.backward()\n",
    "\n",
    "            optimizer_norbert.step()\n",
    "            optimizer2.step()\n",
    "            lr_scheduler_norbert.step()\n",
    "            lr_scheduler2.step()\n",
    "            optimizer_norbert.zero_grad()\n",
    "            optimizer2.zero_grad()\n",
    "        print(f'Epoch {epoch} Loss: {train_loss / train_samples}')\n",
    "        norbert.eval()\n",
    "        model.eval()\n",
    "        total_loss = 0.0\n",
    "        total_samples = 0\n",
    "        # Evaluate the model on the validation set.\n",
    "        for batch in eval_dataloader:\n",
    "                \n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "               \n",
    "                with torch.no_grad():\n",
    "                    outputs = norbert(**batch)\n",
    "\n",
    "                    batch_hidden_states = outputs.last_hidden_state\n",
    "                    model_output =    model(batch_hidden_states)\n",
    "                loss = F.cross_entropy(model_output, batch[\"labels\"])\n",
    "\n",
    "\n",
    "                total_loss += loss * batch['labels'].size(0) \n",
    "                total_samples += batch['labels'].size(0)\n",
    "\n",
    "\n",
    "        average_loss = total_loss / total_samples\n",
    "        print(f'Evaluation Loss: {average_loss}')    \n",
    "\n",
    " \n",
    "        \n",
    "\n",
    "        if average_loss < last:\n",
    "            \n",
    "            Counter = 0\n",
    "            if average_loss < best:\n",
    "                torch.save(model, i)\n",
    "                torch.save(norbert, j)\n",
    "                best = average_loss\n",
    "           \n",
    "        last = average_loss\n",
    "        Counter +=1\n",
    "        if Counter > 2:   \n",
    "                print(epoch)         \n",
    "                break\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90d1f0a-4468-43de-abda-ce3b5038f3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# evaluate the first trained model\n",
    "norbert = torch.load(norbert_models[0])\n",
    "model = torch.load(models[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789c7483",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, classification_report, accuracy_score, recall_score, precision_score\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "norbert.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = norbert(**batch)\n",
    "        batch_hidden_states = outputs.last_hidden_state\n",
    "        model_output =    model(batch_hidden_states)\n",
    "\n",
    "\n",
    "        \n",
    "        logits = model_output\n",
    "        # Calculate predictions\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "        # Collect predictions and labels\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(batch[\"labels\"].cpu().numpy())\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(all_labels, all_predictions, average='macro')\n",
    "ac = accuracy_score(all_labels, all_predictions)\n",
    "recall = recall_score(all_labels, all_predictions, average='macro')\n",
    "precision = precision_score(all_labels, all_predictions, average='macro')\n",
    "print(f\"Test F1 Score: {f1:.3f}\")\n",
    "print(f\"Test Accuracy: {ac:.3f}\")\n",
    "print(f\"Test Recall: {recall:.3f}\")\n",
    "print(f\"Test Precision: {precision:.3f}\")\n",
    "\n",
    "# Optionally, you can print a detailed classification report\n",
    "report = classification_report(all_labels, all_predictions)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd1352b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "# Display the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "c = sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=np.unique(all_labels), yticklabels=np.unique(all_labels))\n",
    "c.collections[0].colorbar.remove()\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show() \n",
    "\n",
    "# Optionally, you can print a detailed classification report\n",
    "report = classification_report(all_labels, all_predictions)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4583e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percentages for each true label\n",
    "total_true_labels = np.sum(conf_matrix, axis=1)\n",
    "percentages = (conf_matrix / total_true_labels[:, np.newaxis]) * 100\n",
    "# Replace NaN values with 0\n",
    "percentages = np.nan_to_num(percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ce2126",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = sns.heatmap(percentages, annot=True, fmt=\"f\", cmap=\"Blues\", xticklabels=np.unique(all_labels), yticklabels=np.unique(all_labels))\n",
    "c.collections[0].colorbar.remove()\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
