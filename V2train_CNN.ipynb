{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf00378-a443-45f9-901f-67e1fad3fb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0944324b",
   "metadata": {},
   "source": [
    "The program can work with any of the four datasets with minor modifications. This program is adapted for the relabeled version of the mixed-label dataset. The program is written and commented with the help of ChatGPT and Copilot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2824b3-f8c1-40ad-b0c8-9fab68963fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Statistikkprosjekt/Mixed\")\n",
    "#dataset = load_dataset(\"ltg/norec_sentence\",\"ternary\")\n",
    "num_classes = 4\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a15ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN_NLP(nn.Module):\n",
    " \n",
    "    def __init__(self,\n",
    "                 \n",
    "                 vocab_size=1024,\n",
    "            \n",
    "                 filter_sizes=[3, 4, 5],\n",
    "                 num_filters=[100, 100, 100],\n",
    "                 num_classes=4,\n",
    "                 dropout=0.5):\n",
    "\n",
    "\n",
    "        super(CNN_NLP, self).__init__()\n",
    "\n",
    "    \n",
    "        # Conv Network\n",
    "        self.conv1d_list = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=vocab_size,\n",
    "                      out_channels=num_filters[i],\n",
    "                      kernel_size=filter_sizes[i])\n",
    "            for i in range(len(filter_sizes))\n",
    "        ])\n",
    "        # Fully-connected layer and Dropout\n",
    "        self.fc = nn.Linear(np.sum(num_filters), num_classes)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "\n",
    "\n",
    "       \n",
    "        x_embed = input_ids.float()\n",
    "\n",
    "\n",
    "        x_reshaped = x_embed.permute(0, 2, 1)\n",
    "\n",
    "        # Apply CNN and ReLU. Output shape: (b, num_filters[i], L_out)\n",
    "        x_conv_list = [F.relu(conv1d(x_reshaped)) for conv1d in self.conv1d_list]\n",
    "\n",
    "        # Max pooling. Output shape: (b, num_filters[i], 1)\n",
    "        x_pool_list = [F.max_pool1d(x_conv, kernel_size=x_conv.shape[2])\n",
    "            for x_conv in x_conv_list]\n",
    "        \n",
    "        # Concatenate x_pool_list to feed the fully connected layer.\n",
    "        # Output shape: (b, sum(num_filters))\n",
    "        x_fc = torch.cat([x_pool.squeeze(dim=2) for x_pool in x_pool_list],\n",
    "                         dim=1)\n",
    "        \n",
    "        # Compute logits. Output shape: (b, n_classes)\n",
    "        logits = self.fc(self.dropout(x_fc))\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1616fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ltg/norbert3-base\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"review\"], padding=\"max_length\", truncation=True, max_length = 90, return_tensors=\"pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99a2741",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "num_epochs = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798ad752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models is where the BiGRU model will be saved \n",
    "# norbert_models is where the NorBERT model will be saved\n",
    "\n",
    "models = [\n",
    "     \"example_folder/example_model1.pth\", \"example_folder/example_model2.pth\", \"example_folder/example_model3.pth\", \"example_folder/example_model4.pth\", \"example_folder/example_model5.pth\", \"example_folder/example_model6.pth\", \"example_folder/example_model7.pth\", \"example_folder/example_model8.pth\", \"example_folder/example_model9.pth\", \"example_folder/example_model10.pth\", \"example_folder/example_model11.pth\", \"example_folder/example_model12.pth\", \"example_folder/example_model13.pth\", \"example_folder/example_model14.pth\", \"example_folder/example_model15.pth\"\n",
    "          ]\n",
    "        \n",
    "norbert_models = [\n",
    "    \"example_folder/example_norbert1.pth\", \"example_folder/example_norbert2.pth\", \"example_folder/example_norbert3.pth\", \"example_folder/example_norbert4.pth\", \"example_folder/example_norbert5.pth\", \"example_folder/example_norbert6.pth\", \"example_folder/example_norbert7.pth\", \"example_folder/example_norbert8.pth\", \"example_folder/example_norbert9.pth\", \"example_folder/example_norbert10.pth\", \"example_folder/example_norbert11.pth\", \"example_folder/example_norbert12.pth\", \"example_folder/example_norbert13.pth\", \"example_folder/example_norbert14.pth\", \"example_folder/example_norbert15.pth\"\n",
    "       ]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a3a245",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j in zip( models, norbert_models):\n",
    "    last = 199\n",
    "    best = 199\n",
    "    print(i)\n",
    "\n",
    "\n",
    "    tokenized_datasets= dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "    tokenized_datasets  = tokenized_datasets.remove_columns([\"review\"])\n",
    "\n",
    "    tokenized_datasets = tokenized_datasets.rename_column(\"polarity\", \"labels\")\n",
    "\n",
    "    tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "    train_dataloader = DataLoader(tokenized_datasets[\"train\"], shuffle=True, batch_size=14)\n",
    "    eval_dataloader = DataLoader(tokenized_datasets[\"validation\"], batch_size=10)\n",
    "    num_training_steps = 10 * len(train_dataloader)\n",
    "\n",
    "    norbert = AutoModel.from_pretrained(\"ltg/norbert3-large\", trust_remote_code=True)\n",
    "\n",
    "    text = \"Your input text goes here.\"\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "   \n",
    "    with torch.no_grad():\n",
    "        outputs_bert = norbert(**tokens)\n",
    "\n",
    "    last_hidden_states = outputs_bert.last_hidden_state\n",
    "\n",
    "    cls_embeddings = last_hidden_states[:, 0, :]\n",
    "\n",
    "    bert_hidden_size = last_hidden_states.size(-1)\n",
    "\n",
    "    model = CNN_NLP(vocab_size=bert_hidden_size, num_classes=num_classes, dropout=0.5, filter_sizes=[2,3,4], num_filters=[200, 200, 200])\n",
    "    \n",
    "    # Optimizer and learning rate scheduler\n",
    "    optimizer_norbert = AdamW(norbert.parameters(), lr=6e-6) \n",
    "    optimizer2 = AdamW(model.parameters(), lr=1e-5)  \n",
    "    lr_scheduler_norbert = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer_norbert, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "    lr_scheduler2 = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer2, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "    # Move the model to the GPU\n",
    "    norbert.to(device)\n",
    "    model.to(device)\n",
    "\n",
    "   # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "      \n",
    "        average_loss = 0   \n",
    "        norbert.train()\n",
    "        model.train()   \n",
    "        train_loss = 0\n",
    "        train_samples = 0\n",
    "        for batch in train_dataloader:\n",
    "            \n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = norbert(**batch)\n",
    "\n",
    "            batch_hidden_states = outputs.last_hidden_state\n",
    "            model_output =    model(batch_hidden_states)\n",
    "            loss = F.cross_entropy(model_output, batch[\"labels\"])\n",
    "      \n",
    "\n",
    "\n",
    "            train_loss += loss * batch['labels'].size(0)  \n",
    "            train_samples += batch['labels'].size(0)\n",
    "\n",
    "           \n",
    "            \n",
    "            loss.backward()\n",
    "\n",
    "            optimizer_norbert.step()\n",
    "            optimizer2.step()\n",
    "            lr_scheduler_norbert.step()\n",
    "            lr_scheduler2.step()\n",
    "            optimizer_norbert.zero_grad()\n",
    "            optimizer2.zero_grad()\n",
    "        print(f'Epoch {epoch} Loss: {train_loss / train_samples}')\n",
    "        norbert.eval()\n",
    "        model.eval()\n",
    "        total_loss = 0.0\n",
    "        total_samples = 0\n",
    "        # Evaluate the model on the validation set\n",
    "        for batch in eval_dataloader:\n",
    "                \n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "               \n",
    "                with torch.no_grad():\n",
    "                    outputs = norbert(**batch)\n",
    "\n",
    "                    batch_hidden_states = outputs.last_hidden_state\n",
    "                    model_output =    model(batch_hidden_states)\n",
    "                loss = F.cross_entropy(model_output, batch[\"labels\"])\n",
    "\n",
    "\n",
    "                total_loss += loss * batch['labels'].size(0) \n",
    "                total_samples += batch['labels'].size(0)\n",
    "\n",
    "\n",
    "        average_loss = total_loss / total_samples\n",
    "        print(f'Evaluation Loss: {average_loss}')    \n",
    "\n",
    " \n",
    "        \n",
    "\n",
    "        if average_loss < last:\n",
    "            \n",
    "            Counter = 0\n",
    "            if average_loss < best:\n",
    "                torch.save(model, i)\n",
    "                torch.save(norbert, j)\n",
    "                best = average_loss\n",
    "           \n",
    "        last = average_loss\n",
    "        Counter +=1\n",
    "        if Counter > 2:   \n",
    "                print(epoch)         \n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90d1f0a-4468-43de-abda-ce3b5038f3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Load the model first trained model\n",
    "norbert = torch.load(norbert_models[0])\n",
    "model = torch.load(models[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789c7483",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, classification_report, accuracy_score, recall_score, precision_score\n",
    "\n",
    "model.eval()\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "# evaluate the model\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = norbert(**batch)\n",
    "        batch_hidden_states = outputs.last_hidden_state\n",
    "        model_output =    model(batch_hidden_states)\n",
    "\n",
    "    \n",
    "        logits = model_output\n",
    " \n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(batch[\"labels\"].cpu().numpy())\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(all_labels, all_predictions, average='macro')\n",
    "ac = accuracy_score(all_labels, all_predictions)\n",
    "recall = recall_score(all_labels, all_predictions, average='macro')\n",
    "precision = precision_score(all_labels, all_predictions, average='macro')\n",
    "print(f\"Test F1 Score: {f1:.3f}\")\n",
    "print(f\"Test Accuracy: {ac:.3f}\")\n",
    "print(f\"Test Recall: {recall:.3f}\")\n",
    "print(f\"Test Precision: {precision:.3f}\")\n",
    "\n",
    "# Optionally, you can print a detailed classification report\n",
    "report = classification_report(all_labels, all_predictions)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd1352b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "# Display the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "c = sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=np.unique(all_labels), yticklabels=np.unique(all_labels))\n",
    "c.collections[0].colorbar.remove()\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show() \n",
    "\n",
    "# Optionally, you can print a detailed classification report\n",
    "report = classification_report(all_labels, all_predictions)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4583e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percentages for each true label\n",
    "total_true_labels = np.sum(conf_matrix, axis=1)\n",
    "percentages = (conf_matrix / total_true_labels[:, np.newaxis]) * 100\n",
    "\n",
    "# Replace NaN values with 0 (for cases where the true label count is 0)\n",
    "percentages = np.nan_to_num(percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ce2126",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = sns.heatmap(percentages, annot=True, fmt=\"f\", cmap=\"Blues\", xticklabels=np.unique(all_labels), yticklabels=np.unique(all_labels))\n",
    "c.collections[0].colorbar.remove()\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
